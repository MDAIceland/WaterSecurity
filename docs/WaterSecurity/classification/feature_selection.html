<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>modern_data_analytics.classification.feature_selection API documentation</title>
<meta name="description" content="- - - Feature Generation and Feature Selection Script - - -
We use this script to generate new features from already existing ones.
Throughout this â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>modern_data_analytics.classification.feature_selection</code></h1>
</header>
<section id="section-intro">
<ul>
<li>
<ul>
<li>
<ul>
<li>Feature Generation and Feature Selection Script - - -
We use this script to generate new features from already existing ones.
Throughout this process, generation of the new features are done by using
PCA and Polynomial Cross Features algorithm. Once feature generation is done,
script uses all of the generated and original features as an input to perform
a feature selection based on the SelectKBest algorithm of the Sklearn. F_regression
score is used since numbers in the risk factors are representing a certain value.
In the end, only selected feature columns, latitude and longitude columns are returned
back for further prediction of the NaN values for a specific risk factor.</li>
</ul>
</li>
</ul>
</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
- - - Feature Generation and Feature Selection Script - - -
We use this script to generate new features from already existing ones.
Throughout this process, generation of the new features are done by using 
PCA and Polynomial Cross Features algorithm. Once feature generation is done,
script uses all of the generated and original features as an input to perform
a feature selection based on the SelectKBest algorithm of the Sklearn. F_regression
score is used since numbers in the risk factors are representing a certain value.
In the end, only selected feature columns, latitude and longitude columns are returned
back for further prediction of the NaN values for a specific risk factor.
&#39;&#39;&#39;


from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import PolynomialFeatures, RobustScaler
from sklearn.decomposition import PCA
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np


# Generation of the Feature Selection Class for the Pipeline
class FeatureSelection(BaseEstimator, TransformerMixin):

    # Initiation of the variables for the feature selection, using sklearn SelectKBest Algorithm
    def __init__(self):
        self.fitted_selector = None
        self.min_num_feats = 10
        self.scores_ = None

    # Process of feature selection is done in this part: Select the K-best features
    # using the f_regression function since labels also have a meaning Risk:0 (means low) and 2(means high)
    def fit(self, x, y):
        # risks.remove(label)
        # print(risks)
        assert all(~np.isnan(y))

        # l = x.columns[x.isna().any()].tolist()
        # print(l)

        var_num = x.shape[0]
        var_num = max(int((var_num * 15) / 100), self.min_num_feats)
        print(&#34;Picked variable number:&#34;, var_num)

        # Applying select K-best
        bestFeatures = SelectKBest(score_func=f_regression, k=var_num)
        self.fitted_selector = bestFeatures.fit(x, y)
        self.scores_ = self.fitted_selector.scores_
        self.feats_indices = bestFeatures.get_support()

    # Return the vector of best features
    def transform(self, X):
        return self.fitted_selector.transform(X)


# Generation of the transformer class for the return of selected features in the pipeline
class DummyTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.feats = None

    def fit(self, X, y):
        self.feats = X.columns
        return self

    def transform(self, X):
        return X

    def get_feature_names(self):
        return self.feats


import re

# Class for generation of cross polynomial features
class ColumnSubstringPolynomial(BaseEstimator, TransformerMixin):

    def __init__(self, element):
        self.element = element
        self.poly = None
        self.pop_feats = []

    # Returns every column name that has a specific string inside
    @staticmethod
    def getArrayOfFeatures(data, name):
        arr = data.columns.values
        return [s for s in arr if (name in s)]

    # Obtaining the list of columns and fitting them to a feature generation model
    def fit(self, X, y=None):
        self.poly = PolynomialFeatures(interaction_only=False, include_bias=False)
        self.pop_feats = self.getArrayOfFeatures(X, self.element)
        self.poly.fit(X[self.pop_feats].values)
        self.mask = np.sum(self.poly.powers_, axis=1) &gt; 1

        # print(crossed_df.shape)
        return self

    # Acquire the selected features and generate a dataframe with only selected feature columns
    def transform(self, data):
        crossed_feats = self.poly.transform(data[self.pop_feats])[:, self.mask]

        # Convert to Pandas DataFrame and merge to original dataset
        crossed_df = pd.DataFrame(crossed_feats)
        return crossed_df

    # Obtain the original feature names after the automatic naming of the cross feature algorithm
    def get_feature_names(self):
        feats = []
        replacement_dict = {cnt: x for cnt, x in enumerate(self.pop_feats)}
        comb_pattern = re.compile(r&#34;(x[0-9]+) (x[0-9]+)&#34;)
        single_pattern = re.compile(r&#34;(x[0-9]+)&#34;)
        pattern = re.compile(r&#34;(x[0-9]+)&#34;)
        for feat in np.array(self.poly.get_feature_names())[self.mask]:
            if re.match(comb_pattern, feat):
                out_feat = re.sub(
                    comb_pattern,
                    r&#34;Feat[\1] * Feat[\2]&#34;,
                    feat,
                )
            else:
                out_feat = re.sub(
                    single_pattern,
                    r&#34;Feat[\1]&#34;,
                    feat,
                )
            out_feat = re.sub(
                pattern,
                lambda m: replacement_dict[int(m.group()[1:])],
                out_feat,
            )
            feats.append(out_feat)
        return feats


# Generate a PCA class for using the pipeline
class PCAWrapper(BaseEstimator, TransformerMixin):

    # Initializing necessary variables for performing the PCA
    def __init__(self):
        self.num_components = 10
        self.pca = PCA(n_components=self.num_components)
        self.component_cols = [&#34;PC&#34; + str(i + 1) for i in range(self.num_components)]

    # Fit the data to Principle Component Generator, return insight regarding the
    # first 10 principle components of the dataset
    def fit(self, X, y):
        self.pca.fit(X)
        percentage_list = [
            element * 100 for element in self.pca.explained_variance_ratio_
        ]

        print(
            &#34;Explained variation percentage per principal component: {}&#34;.format(
                percentage_list
            )
        )
        total_explained_percentage = sum(self.pca.explained_variance_ratio_) * 100
        print(
            &#34;Total percentage of the explained data by&#34;,
            self.pca.n_components,
            &#34;components is: %.2f&#34; % total_explained_percentage,
        )
        print(
            &#34;Percentage of the information that is lost for using&#34;,
            self.pca.n_components,
            &#34;components is: %.2f&#34; % (100 - total_explained_percentage),
        )
        return self

    # Return the dataframe of generated first 10 Principle Components
    def transform(self, X):
        pca_ret = self.pca.transform(X)
        return pd.DataFrame(data=pca_ret, columns=self.component_cols)

    # Get the name of the Principle Components
    def get_feature_names(self):
        return self.component_cols


# Robust Scaler Class for the generation of pipeline
class RobustScalerWrapper:

    # Initiation of the Robust Scaler Model
    def __init__(self):
        self.robust_scaler = RobustScaler()
        self.columns = None

    # Fit the robust scaler with the given data and returns the scaling version
    def fit(self, X, y):
        self.columns = X.columns
        self.robust_scaler.fit(X, y)
        return self

    # Scaled version of the dataframe is returned for further processing in the pipeline
    def transform(self, X):
        return pd.DataFrame(self.robust_scaler.transform(X), columns=self.columns)

# Main class for the generation of pipeline
class FeatureSelectionAndGeneration(BaseEstimator, TransformerMixin):

    # Determine the columns that needs to be substracted before the feature generation
    def __init__(self, apply_selection=True):
        self.id_columns = [
            &#34;latitude&#34;,
            &#34;longitude&#34;,
        ]

        # Defining the pipeline order given different classes created for the pipeline process
        self.pipeline = Pipeline(
            [
                (&#34;scale&#34;, RobustScalerWrapper()),
                (
                    &#34;generation&#34;,
                    FeatureUnion(
                        [
                            (&#34;scaled&#34;, DummyTransformer()),
                            (&#34;pca&#34;, PCAWrapper()),
                            (&#34;pop_poly&#34;, ColumnSubstringPolynomial(&#34;population&#34;)),
                            (&#34;perc_poly&#34;, ColumnSubstringPolynomial(&#34;%&#34;)),
                        ]
                    ),
                ),
                (&#34;selection&#34;, FeatureSelection()),
            ]
        )
        self.feat_names = None
        self.apply_selection = apply_selection

        # If feature selection is not applied, you can remove the steps from the pipeline. 
        # Flexible solution to remove the unwanted steps
        if not apply_selection:
            self.pipeline.steps.pop(2)

    # Splitting the initial dataset columns into two, for further processing
    def split(self, data):
        return (
            data[self.id_columns],
            data[[col for col in data.columns.values if col not in self.id_columns]],
        )

    # General order of the feature selection and generation process is defined here
    def fit(self, x_data, y_data):
        &#34;&#34;&#34;
        Fits to nxm features x_data and n predictions y_data
        &#34;&#34;&#34;
        _, x_data = self.split(x_data)
        self.pipeline.fit(x_data, y_data)
        columns = self.pipeline.named_steps[&#34;generation&#34;].get_feature_names()
        dfcolumns = pd.DataFrame(columns)

        # If feature selection process is wanted
        if self.apply_selection:
            dfscores = pd.DataFrame(self.pipeline.named_steps[&#34;selection&#34;].scores_)
            feats_indices = self.pipeline.named_steps[&#34;selection&#34;].feats_indices
            # print(dfscores)

            # Concat two dataframes for better visualization
            featureScores = pd.concat([dfcolumns, dfscores], axis=1)
            featureScores.columns = [&#34;Specs&#34;, &#34;Score&#34;]

            # Print defined amount of features according to the assigned scores with descending order
            print(
                &#34;Features select \n&#34;,
                featureScores.iloc[feats_indices]
                .sort_values(&#34;Score&#34;, ascending=False)
                .to_markdown(),
            )
            self.feat_names = featureScores.iloc[feats_indices].Specs.tolist()
        else:
            self.feat_names = columns
        return self

    # Only return the designated features in addition to the removed features
    # at the beginning of the pipeline process
    def transform(self, x_data):
        &#34;&#34;&#34;
        Transforms x_data from nxm to kxm
        &#34;&#34;&#34;
        labs, x_data = self.split(x_data)
        new_x_data = pd.DataFrame(
            self.pipeline.transform(x_data), columns=self.feat_names
        )

        return pd.concat([labs, new_x_data], axis=1)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial"><code class="flex name class">
<span>class <span class="ident">ColumnSubstringPolynomial</span></span>
<span>(</span><span>element)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ColumnSubstringPolynomial(BaseEstimator, TransformerMixin):

    def __init__(self, element):
        self.element = element
        self.poly = None
        self.pop_feats = []

    # Returns every column name that has a specific string inside
    @staticmethod
    def getArrayOfFeatures(data, name):
        arr = data.columns.values
        return [s for s in arr if (name in s)]

    # Obtaining the list of columns and fitting them to a feature generation model
    def fit(self, X, y=None):
        self.poly = PolynomialFeatures(interaction_only=False, include_bias=False)
        self.pop_feats = self.getArrayOfFeatures(X, self.element)
        self.poly.fit(X[self.pop_feats].values)
        self.mask = np.sum(self.poly.powers_, axis=1) &gt; 1

        # print(crossed_df.shape)
        return self

    # Acquire the selected features and generate a dataframe with only selected feature columns
    def transform(self, data):
        crossed_feats = self.poly.transform(data[self.pop_feats])[:, self.mask]

        # Convert to Pandas DataFrame and merge to original dataset
        crossed_df = pd.DataFrame(crossed_feats)
        return crossed_df

    # Obtain the original feature names after the automatic naming of the cross feature algorithm
    def get_feature_names(self):
        feats = []
        replacement_dict = {cnt: x for cnt, x in enumerate(self.pop_feats)}
        comb_pattern = re.compile(r&#34;(x[0-9]+) (x[0-9]+)&#34;)
        single_pattern = re.compile(r&#34;(x[0-9]+)&#34;)
        pattern = re.compile(r&#34;(x[0-9]+)&#34;)
        for feat in np.array(self.poly.get_feature_names())[self.mask]:
            if re.match(comb_pattern, feat):
                out_feat = re.sub(
                    comb_pattern,
                    r&#34;Feat[\1] * Feat[\2]&#34;,
                    feat,
                )
            else:
                out_feat = re.sub(
                    single_pattern,
                    r&#34;Feat[\1]&#34;,
                    feat,
                )
            out_feat = re.sub(
                pattern,
                lambda m: replacement_dict[int(m.group()[1:])],
                out_feat,
            )
            feats.append(out_feat)
        return feats</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.getArrayOfFeatures"><code class="name flex">
<span>def <span class="ident">getArrayOfFeatures</span></span>(<span>data, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def getArrayOfFeatures(data, name):
    arr = data.columns.values
    return [s for s in arr if (name in s)]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    self.poly = PolynomialFeatures(interaction_only=False, include_bias=False)
    self.pop_feats = self.getArrayOfFeatures(X, self.element)
    self.poly.fit(X[self.pop_feats].values)
    self.mask = np.sum(self.poly.powers_, axis=1) &gt; 1

    # print(crossed_df.shape)
    return self</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.get_feature_names"><code class="name flex">
<span>def <span class="ident">get_feature_names</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_names(self):
    feats = []
    replacement_dict = {cnt: x for cnt, x in enumerate(self.pop_feats)}
    comb_pattern = re.compile(r&#34;(x[0-9]+) (x[0-9]+)&#34;)
    single_pattern = re.compile(r&#34;(x[0-9]+)&#34;)
    pattern = re.compile(r&#34;(x[0-9]+)&#34;)
    for feat in np.array(self.poly.get_feature_names())[self.mask]:
        if re.match(comb_pattern, feat):
            out_feat = re.sub(
                comb_pattern,
                r&#34;Feat[\1] * Feat[\2]&#34;,
                feat,
            )
        else:
            out_feat = re.sub(
                single_pattern,
                r&#34;Feat[\1]&#34;,
                feat,
            )
        out_feat = re.sub(
            pattern,
            lambda m: replacement_dict[int(m.group()[1:])],
            out_feat,
        )
        feats.append(out_feat)
    return feats</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, data):
    crossed_feats = self.poly.transform(data[self.pop_feats])[:, self.mask]

    # Convert to Pandas DataFrame and merge to original dataset
    crossed_df = pd.DataFrame(crossed_feats)
    return crossed_df</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.DummyTransformer"><code class="flex name class">
<span>class <span class="ident">DummyTransformer</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DummyTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.feats = None

    def fit(self, X, y):
        self.feats = X.columns
        return self

    def transform(self, X):
        return X

    def get_feature_names(self):
        return self.feats</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.DummyTransformer.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y):
    self.feats = X.columns
    return self</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.DummyTransformer.get_feature_names"><code class="name flex">
<span>def <span class="ident">get_feature_names</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_names(self):
    return self.feats</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.DummyTransformer.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    return X</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelection"><code class="flex name class">
<span>class <span class="ident">FeatureSelection</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelection(BaseEstimator, TransformerMixin):

    # Initiation of the variables for the feature selection, using sklearn SelectKBest Algorithm
    def __init__(self):
        self.fitted_selector = None
        self.min_num_feats = 10
        self.scores_ = None

    # Process of feature selection is done in this part: Select the K-best features
    # using the f_regression function since labels also have a meaning Risk:0 (means low) and 2(means high)
    def fit(self, x, y):
        # risks.remove(label)
        # print(risks)
        assert all(~np.isnan(y))

        # l = x.columns[x.isna().any()].tolist()
        # print(l)

        var_num = x.shape[0]
        var_num = max(int((var_num * 15) / 100), self.min_num_feats)
        print(&#34;Picked variable number:&#34;, var_num)

        # Applying select K-best
        bestFeatures = SelectKBest(score_func=f_regression, k=var_num)
        self.fitted_selector = bestFeatures.fit(x, y)
        self.scores_ = self.fitted_selector.scores_
        self.feats_indices = bestFeatures.get_support()

    # Return the vector of best features
    def transform(self, X):
        return self.fitted_selector.transform(X)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelection.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x, y):
    # risks.remove(label)
    # print(risks)
    assert all(~np.isnan(y))

    # l = x.columns[x.isna().any()].tolist()
    # print(l)

    var_num = x.shape[0]
    var_num = max(int((var_num * 15) / 100), self.min_num_feats)
    print(&#34;Picked variable number:&#34;, var_num)

    # Applying select K-best
    bestFeatures = SelectKBest(score_func=f_regression, k=var_num)
    self.fitted_selector = bestFeatures.fit(x, y)
    self.scores_ = self.fitted_selector.scores_
    self.feats_indices = bestFeatures.get_support()</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelection.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    return self.fitted_selector.transform(X)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration"><code class="flex name class">
<span>class <span class="ident">FeatureSelectionAndGeneration</span></span>
<span>(</span><span>apply_selection=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelectionAndGeneration(BaseEstimator, TransformerMixin):

    # Determine the columns that needs to be substracted before the feature generation
    def __init__(self, apply_selection=True):
        self.id_columns = [
            &#34;latitude&#34;,
            &#34;longitude&#34;,
        ]

        # Defining the pipeline order given different classes created for the pipeline process
        self.pipeline = Pipeline(
            [
                (&#34;scale&#34;, RobustScalerWrapper()),
                (
                    &#34;generation&#34;,
                    FeatureUnion(
                        [
                            (&#34;scaled&#34;, DummyTransformer()),
                            (&#34;pca&#34;, PCAWrapper()),
                            (&#34;pop_poly&#34;, ColumnSubstringPolynomial(&#34;population&#34;)),
                            (&#34;perc_poly&#34;, ColumnSubstringPolynomial(&#34;%&#34;)),
                        ]
                    ),
                ),
                (&#34;selection&#34;, FeatureSelection()),
            ]
        )
        self.feat_names = None
        self.apply_selection = apply_selection

        # If feature selection is not applied, you can remove the steps from the pipeline. 
        # Flexible solution to remove the unwanted steps
        if not apply_selection:
            self.pipeline.steps.pop(2)

    # Splitting the initial dataset columns into two, for further processing
    def split(self, data):
        return (
            data[self.id_columns],
            data[[col for col in data.columns.values if col not in self.id_columns]],
        )

    # General order of the feature selection and generation process is defined here
    def fit(self, x_data, y_data):
        &#34;&#34;&#34;
        Fits to nxm features x_data and n predictions y_data
        &#34;&#34;&#34;
        _, x_data = self.split(x_data)
        self.pipeline.fit(x_data, y_data)
        columns = self.pipeline.named_steps[&#34;generation&#34;].get_feature_names()
        dfcolumns = pd.DataFrame(columns)

        # If feature selection process is wanted
        if self.apply_selection:
            dfscores = pd.DataFrame(self.pipeline.named_steps[&#34;selection&#34;].scores_)
            feats_indices = self.pipeline.named_steps[&#34;selection&#34;].feats_indices
            # print(dfscores)

            # Concat two dataframes for better visualization
            featureScores = pd.concat([dfcolumns, dfscores], axis=1)
            featureScores.columns = [&#34;Specs&#34;, &#34;Score&#34;]

            # Print defined amount of features according to the assigned scores with descending order
            print(
                &#34;Features select \n&#34;,
                featureScores.iloc[feats_indices]
                .sort_values(&#34;Score&#34;, ascending=False)
                .to_markdown(),
            )
            self.feat_names = featureScores.iloc[feats_indices].Specs.tolist()
        else:
            self.feat_names = columns
        return self

    # Only return the designated features in addition to the removed features
    # at the beginning of the pipeline process
    def transform(self, x_data):
        &#34;&#34;&#34;
        Transforms x_data from nxm to kxm
        &#34;&#34;&#34;
        labs, x_data = self.split(x_data)
        new_x_data = pd.DataFrame(
            self.pipeline.transform(x_data), columns=self.feat_names
        )

        return pd.concat([labs, new_x_data], axis=1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x_data, y_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits to nxm features x_data and n predictions y_data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x_data, y_data):
    &#34;&#34;&#34;
    Fits to nxm features x_data and n predictions y_data
    &#34;&#34;&#34;
    _, x_data = self.split(x_data)
    self.pipeline.fit(x_data, y_data)
    columns = self.pipeline.named_steps[&#34;generation&#34;].get_feature_names()
    dfcolumns = pd.DataFrame(columns)

    # If feature selection process is wanted
    if self.apply_selection:
        dfscores = pd.DataFrame(self.pipeline.named_steps[&#34;selection&#34;].scores_)
        feats_indices = self.pipeline.named_steps[&#34;selection&#34;].feats_indices
        # print(dfscores)

        # Concat two dataframes for better visualization
        featureScores = pd.concat([dfcolumns, dfscores], axis=1)
        featureScores.columns = [&#34;Specs&#34;, &#34;Score&#34;]

        # Print defined amount of features according to the assigned scores with descending order
        print(
            &#34;Features select \n&#34;,
            featureScores.iloc[feats_indices]
            .sort_values(&#34;Score&#34;, ascending=False)
            .to_markdown(),
        )
        self.feat_names = featureScores.iloc[feats_indices].Specs.tolist()
    else:
        self.feat_names = columns
    return self</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split(self, data):
    return (
        data[self.id_columns],
        data[[col for col in data.columns.values if col not in self.id_columns]],
    )</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, x_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms x_data from nxm to kxm</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, x_data):
    &#34;&#34;&#34;
    Transforms x_data from nxm to kxm
    &#34;&#34;&#34;
    labs, x_data = self.split(x_data)
    new_x_data = pd.DataFrame(
        self.pipeline.transform(x_data), columns=self.feat_names
    )

    return pd.concat([labs, new_x_data], axis=1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.PCAWrapper"><code class="flex name class">
<span>class <span class="ident">PCAWrapper</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PCAWrapper(BaseEstimator, TransformerMixin):

    # Initializing necessary variables for performing the PCA
    def __init__(self):
        self.num_components = 10
        self.pca = PCA(n_components=self.num_components)
        self.component_cols = [&#34;PC&#34; + str(i + 1) for i in range(self.num_components)]

    # Fit the data to Principle Component Generator, return insight regarding the
    # first 10 principle components of the dataset
    def fit(self, X, y):
        self.pca.fit(X)
        percentage_list = [
            element * 100 for element in self.pca.explained_variance_ratio_
        ]

        print(
            &#34;Explained variation percentage per principal component: {}&#34;.format(
                percentage_list
            )
        )
        total_explained_percentage = sum(self.pca.explained_variance_ratio_) * 100
        print(
            &#34;Total percentage of the explained data by&#34;,
            self.pca.n_components,
            &#34;components is: %.2f&#34; % total_explained_percentage,
        )
        print(
            &#34;Percentage of the information that is lost for using&#34;,
            self.pca.n_components,
            &#34;components is: %.2f&#34; % (100 - total_explained_percentage),
        )
        return self

    # Return the dataframe of generated first 10 Principle Components
    def transform(self, X):
        pca_ret = self.pca.transform(X)
        return pd.DataFrame(data=pca_ret, columns=self.component_cols)

    # Get the name of the Principle Components
    def get_feature_names(self):
        return self.component_cols</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.PCAWrapper.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y):
    self.pca.fit(X)
    percentage_list = [
        element * 100 for element in self.pca.explained_variance_ratio_
    ]

    print(
        &#34;Explained variation percentage per principal component: {}&#34;.format(
            percentage_list
        )
    )
    total_explained_percentage = sum(self.pca.explained_variance_ratio_) * 100
    print(
        &#34;Total percentage of the explained data by&#34;,
        self.pca.n_components,
        &#34;components is: %.2f&#34; % total_explained_percentage,
    )
    print(
        &#34;Percentage of the information that is lost for using&#34;,
        self.pca.n_components,
        &#34;components is: %.2f&#34; % (100 - total_explained_percentage),
    )
    return self</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.PCAWrapper.get_feature_names"><code class="name flex">
<span>def <span class="ident">get_feature_names</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_names(self):
    return self.component_cols</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.PCAWrapper.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    pca_ret = self.pca.transform(X)
    return pd.DataFrame(data=pca_ret, columns=self.component_cols)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.RobustScalerWrapper"><code class="flex name class">
<span>class <span class="ident">RobustScalerWrapper</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RobustScalerWrapper:

    # Initiation of the Robust Scaler Model
    def __init__(self):
        self.robust_scaler = RobustScaler()
        self.columns = None

    # Fit the robust scaler with the given data and returns the scaling version
    def fit(self, X, y):
        self.columns = X.columns
        self.robust_scaler.fit(X, y)
        return self

    # Scaled version of the dataframe is returned for further processing in the pipeline
    def transform(self, X):
        return pd.DataFrame(self.robust_scaler.transform(X), columns=self.columns)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="modern_data_analytics.classification.feature_selection.RobustScalerWrapper.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y):
    self.columns = X.columns
    self.robust_scaler.fit(X, y)
    return self</code></pre>
</details>
</dd>
<dt id="modern_data_analytics.classification.feature_selection.RobustScalerWrapper.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    return pd.DataFrame(self.robust_scaler.transform(X), columns=self.columns)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="modern_data_analytics.classification" href="index.html">modern_data_analytics.classification</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial" href="#modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial">ColumnSubstringPolynomial</a></code></h4>
<ul class="">
<li><code><a title="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.fit" href="#modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.fit">fit</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.getArrayOfFeatures" href="#modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.getArrayOfFeatures">getArrayOfFeatures</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.get_feature_names" href="#modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.get_feature_names">get_feature_names</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.transform" href="#modern_data_analytics.classification.feature_selection.ColumnSubstringPolynomial.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="modern_data_analytics.classification.feature_selection.DummyTransformer" href="#modern_data_analytics.classification.feature_selection.DummyTransformer">DummyTransformer</a></code></h4>
<ul class="">
<li><code><a title="modern_data_analytics.classification.feature_selection.DummyTransformer.fit" href="#modern_data_analytics.classification.feature_selection.DummyTransformer.fit">fit</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.DummyTransformer.get_feature_names" href="#modern_data_analytics.classification.feature_selection.DummyTransformer.get_feature_names">get_feature_names</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.DummyTransformer.transform" href="#modern_data_analytics.classification.feature_selection.DummyTransformer.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelection" href="#modern_data_analytics.classification.feature_selection.FeatureSelection">FeatureSelection</a></code></h4>
<ul class="">
<li><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelection.fit" href="#modern_data_analytics.classification.feature_selection.FeatureSelection.fit">fit</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelection.transform" href="#modern_data_analytics.classification.feature_selection.FeatureSelection.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration" href="#modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration">FeatureSelectionAndGeneration</a></code></h4>
<ul class="">
<li><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.fit" href="#modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.fit">fit</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.split" href="#modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.split">split</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.transform" href="#modern_data_analytics.classification.feature_selection.FeatureSelectionAndGeneration.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="modern_data_analytics.classification.feature_selection.PCAWrapper" href="#modern_data_analytics.classification.feature_selection.PCAWrapper">PCAWrapper</a></code></h4>
<ul class="">
<li><code><a title="modern_data_analytics.classification.feature_selection.PCAWrapper.fit" href="#modern_data_analytics.classification.feature_selection.PCAWrapper.fit">fit</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.PCAWrapper.get_feature_names" href="#modern_data_analytics.classification.feature_selection.PCAWrapper.get_feature_names">get_feature_names</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.PCAWrapper.transform" href="#modern_data_analytics.classification.feature_selection.PCAWrapper.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="modern_data_analytics.classification.feature_selection.RobustScalerWrapper" href="#modern_data_analytics.classification.feature_selection.RobustScalerWrapper">RobustScalerWrapper</a></code></h4>
<ul class="">
<li><code><a title="modern_data_analytics.classification.feature_selection.RobustScalerWrapper.fit" href="#modern_data_analytics.classification.feature_selection.RobustScalerWrapper.fit">fit</a></code></li>
<li><code><a title="modern_data_analytics.classification.feature_selection.RobustScalerWrapper.transform" href="#modern_data_analytics.classification.feature_selection.RobustScalerWrapper.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>