<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>water_security.classification.model_handler API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>water_security.classification.model_handler</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from sklearn.base import BaseEstimator
from data.labeled.preprocessed import LABELED_CITIES
import os
import pickle
import pandas as pd
import numpy as np
import importlib
from data.model import MODEL_PATH
from sklearn.pipeline import Pipeline
from classification.classifier import Classifier
from classification.feature_selection import FeatureSelectionAndGeneration
from sklearn.exceptions import NotFittedError
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from classification import RANDOM_SEED
from data.model.metrics import VALIDATION_METRICS_PATH, TRAINING_METRICS_PATH
from data.model.predictions import PREDICTION_MASK_PATH, FILLED_DATASET_PATH
from utils.geo import is_close, get_place, get_average_1k_population_density


class TrainingRequired(NotFittedError):
    def __init__(self, obj):
        super().__init__(f&#34;{obj} could not be loaded. Training model is required&#34;)


class InvalidCoordinates(BaseException):
    pass


class ModelHandler:
    &#34;&#34;&#34;
    Trains and Tests the model, while also computing metrics.
    During training the model is first fitted, then produces predictions for any unlabled points inside the dataset
    &#34;&#34;&#34;

    def __init__(self):
        self._model = None
        self._dataset = None
        self._valid_metrics = None
        self._train_metrics = None
        self._filled_dataset = None
        self.train_mask = None
        self.feat_names = None
        self.lab_names = None
        # The id columns to remain in the filled dataset
        self.id_columns = [&#34;city&#34;, &#34;country&#34;, &#34;country_code&#34;, &#34;c40&#34;]

    @property
    def model(self) -&gt; Pipeline:
        &#34;&#34;&#34;
        If model is not defined, try to loaded from disk
        &#34;&#34;&#34;
        if self._model is None:
            try:
                from data.model import MODEL

                self._model = MODEL
            except ImportError:
                raise TrainingRequired(&#34;Model&#34;)
        return self._model

    @model.setter
    def model(self, model: Pipeline):
        self._model = model

    def save_model(self) -&gt; None:
        &#34;&#34;&#34;
        Saves model to memory
        &#34;&#34;&#34;
        with open(os.path.join(MODEL_PATH), &#34;wb&#34;) as out:
            pickle.dump(self.model, out)
        import data.model

        importlib.reload(data.model)

    @property
    def dataset(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        The dataset for the training step.
        When it is loaded the first time, several variables are defined:
            - lab_names: the labels names/columns of the dataset
            - unique_labs: the unique labels values
            - feat_names: the features names/columns of the dataset
            - train_mask: the mask that refers to the cities that are labeled at least for one risk
        &#34;&#34;&#34;
        if self._dataset is None:
            from data.labeled.preprocessed import LABELED_CITIES, RISKS_MAPPING
            from data.dataset import DATASET as dataset

            self.lab_names = sorted(RISKS_MAPPING.keys())
            self.unique_labs = np.unique(dataset[self.lab_names].T.stack().values)
            self.feat_names = [
                x
                for x in dataset.columns
                if x not in self.lab_names and x not in self.id_columns
            ]
            self.train_mask = dataset[self.lab_names].apply(
                lambda x: all(pd.isnull(x)), axis=1
            )
            self._dataset = dataset
        return self._dataset

    @property
    def filled_dataset(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        The dataset that has filled labels, which were produced from the predictions
        &#34;&#34;&#34;
        if self._filled_dataset is None:
            try:
                self._filled_dataset = pd.read_csv(FILLED_DATASET_PATH)
            except IOError:
                raise TrainingRequired(&#34;Filled Dataset&#34;)
        return self._filled_dataset

    @filled_dataset.setter
    def filled_dataset(self, dataset: pd.DataFrame):
        self._filled_dataset = dataset

    def compute_metrics(self, y_true, y_pred):
        &#34;&#34;&#34;
        Compute metrics for regression labels of size nx1
        &#34;&#34;&#34;
        metrics = {}
        # Interpolate predictions to labels, eg convert 0.2 to 0, 0.7 to 1 etc.
        y_pred_interp = self.unique_labs[
            np.abs(np.reshape(self.unique_labs, (-1, 1)) - y_pred).argmin(axis=0)
        ]
        metrics[&#34;confusion_matrix&#34;] = confusion_matrix(y_true, y_pred_interp)
        metrics[&#34;classification_report&#34;] = classification_report(y_true, y_pred)
        return metrics

    @property
    def is_fitted(self) -&gt; bool:
        &#34;&#34;&#34;
        Tries to load model from memory/disk, if it fails, returns False, else returns True
        &#34;&#34;&#34;
        try:
            self.model
        except TrainingRequired:
            return False
        return True

    def train(self) -&gt; None:
        &#34;&#34;&#34;
        - Trains 7 different models, one per each different water security risk.
        - Applies feature selection and generation per different model.
        - Keeps 0.3 validation size, computes classification metrics, saves them, then fits each model to the whole available dataset for each risk.
        - Creates the filled dataset and saves it to disk
        - Creates the prediction mask (what labels from the filled dataset were predicted) and saves it to memory
        &#34;&#34;&#34;
        dataset = self.dataset
        labeled = dataset[self.train_mask]
        model = {}
        train_metrics = {}
        valid_metrics = {}
        filled_dataset = dataset[self.id_columns + self.lab_names].copy()
        for label in self.lab_names:
            train_mask = ~pd.isnull(dataset[label])
            labeled = dataset.loc[train_mask, :]
            train_set, valid_set = train_test_split(
                labeled, test_size=0.3, random_state=RANDOM_SEED
            )

            model[label] = Pipeline(
                [
                    (&#34;FeatureSelection&#34;, FeatureSelectionAndGeneration()),
                    (&#34;Classification&#34;, Classifier()),
                ]
            )
            model[label].fit(train_set[self.feat_names], train_set[label])
            train_preds = model[label].predict(train_set[self.feat_names])
            valid_preds = model[label].predict(valid_set[self.feat_names])
            train_metrics[label] = self.compute_metrics(train_set[label], train_preds)
            valid_metrics[label] = self.compute_metrics(valid_set[label], valid_preds)
            model[label].fit(labeled[self.feat_names], labeled[label])

            filled_dataset.loc[~train_mask, label] = model[label].predict(
                dataset.loc[~train_mask, self.feat_names]
            )
        self.model = model
        self.save_model()
        with open(VALIDATION_METRICS_PATH, &#34;wb&#34;) as out:
            pickle.dump(valid_metrics, out)
        with open(TRAINING_METRICS_PATH, &#34;wb&#34;) as out:
            pickle.dump(train_metrics, out)
        self.filled_dataset = filled_dataset
        self.filled_dataset.to_csv(FILLED_DATASET_PATH, index=False)
        prediction_mask = self.filled_dataset[self.id_columns + self.lab_names]
        prediction_mask[self.lab_names] = pd.isnull(dataset[self.lab_names])
        prediction_mask.to_csv(PREDICTION_MASK_PATH, index=False)
        import data.model.metrics

        importlib.reload(data.model.metrics)
        import data.model.predictions

        importlib.reload(data.model.predictions)

    def test(self, latitude, longitude):
        &#34;&#34;&#34;
        Given a specific latitude and longitude value, either returns saved predictions from the filled dataset, if the point is close to the
        ones that have already been predicted, or uses a REST API to load the country to which the latitude and longitude refer, uses the country data
        to create the feature vector and computes the prediction using the trained models.
        Returns the series of the found labels, which also contain city and country, and the series of booleans which shows which predictions were predicted and which where not.
        &#34;&#34;&#34;
        try:
            from data.model.predictions import FILLED_DATASET, PREDICTION_MASK
        except ImportError:
            raise TrainingRequired(&#34;Filled Dataset&#34;)
        check_existing = FILLED_DATASET.apply(
            lambda x: is_close((latitude, longitude), (x[&#34;latitude&#34;], x[&#34;longitude&#34;])),
            axis=1,
        )
        if np.any(check_existing):
            labs = list(sorted(self.model.keys()))
            return (
                FILLED_DATASET.loc[check_existing, labs + [&#34;city&#34;, &#34;country&#34;]].iloc[0],
                PREDICTION_MASK.loc[check_existing, labs].iloc[0],
            )
        try:
            place = get_place(latitude, longitude)
        except AttributeError:
            raise InvalidCoordinates
        population_density = get_average_1k_population_density(latitude, longitude)

        from data.unlabeled import COUNTRIES_DATASET

        feats = COUNTRIES_DATASET.loc[place[&#34;code&#34;]].copy()
        feats[&#34;population_1k_density&#34;] = population_density
        preds = {}
        mask = {}
        for label in self.model:
            preds[label] = self.model[label].predict(feats)[0]
            mask[label] = True
        preds[&#34;city&#34;] = place[&#34;city&#34;]
        preds[&#34;country&#34;] = place[&#34;country&#34;]
        return pd.Series(preds), pd.Series(mask)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="water_security.classification.model_handler.InvalidCoordinates"><code class="flex name class">
<span>class <span class="ident">InvalidCoordinates</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all exceptions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InvalidCoordinates(BaseException):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler"><code class="flex name class">
<span>class <span class="ident">ModelHandler</span></span>
</code></dt>
<dd>
<div class="desc"><p>Trains and Tests the model, while also computing metrics.
During training the model is first fitted, then produces predictions for any unlabled points inside the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelHandler:
    &#34;&#34;&#34;
    Trains and Tests the model, while also computing metrics.
    During training the model is first fitted, then produces predictions for any unlabled points inside the dataset
    &#34;&#34;&#34;

    def __init__(self):
        self._model = None
        self._dataset = None
        self._valid_metrics = None
        self._train_metrics = None
        self._filled_dataset = None
        self.train_mask = None
        self.feat_names = None
        self.lab_names = None
        # The id columns to remain in the filled dataset
        self.id_columns = [&#34;city&#34;, &#34;country&#34;, &#34;country_code&#34;, &#34;c40&#34;]

    @property
    def model(self) -&gt; Pipeline:
        &#34;&#34;&#34;
        If model is not defined, try to loaded from disk
        &#34;&#34;&#34;
        if self._model is None:
            try:
                from data.model import MODEL

                self._model = MODEL
            except ImportError:
                raise TrainingRequired(&#34;Model&#34;)
        return self._model

    @model.setter
    def model(self, model: Pipeline):
        self._model = model

    def save_model(self) -&gt; None:
        &#34;&#34;&#34;
        Saves model to memory
        &#34;&#34;&#34;
        with open(os.path.join(MODEL_PATH), &#34;wb&#34;) as out:
            pickle.dump(self.model, out)
        import data.model

        importlib.reload(data.model)

    @property
    def dataset(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        The dataset for the training step.
        When it is loaded the first time, several variables are defined:
            - lab_names: the labels names/columns of the dataset
            - unique_labs: the unique labels values
            - feat_names: the features names/columns of the dataset
            - train_mask: the mask that refers to the cities that are labeled at least for one risk
        &#34;&#34;&#34;
        if self._dataset is None:
            from data.labeled.preprocessed import LABELED_CITIES, RISKS_MAPPING
            from data.dataset import DATASET as dataset

            self.lab_names = sorted(RISKS_MAPPING.keys())
            self.unique_labs = np.unique(dataset[self.lab_names].T.stack().values)
            self.feat_names = [
                x
                for x in dataset.columns
                if x not in self.lab_names and x not in self.id_columns
            ]
            self.train_mask = dataset[self.lab_names].apply(
                lambda x: all(pd.isnull(x)), axis=1
            )
            self._dataset = dataset
        return self._dataset

    @property
    def filled_dataset(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        The dataset that has filled labels, which were produced from the predictions
        &#34;&#34;&#34;
        if self._filled_dataset is None:
            try:
                self._filled_dataset = pd.read_csv(FILLED_DATASET_PATH)
            except IOError:
                raise TrainingRequired(&#34;Filled Dataset&#34;)
        return self._filled_dataset

    @filled_dataset.setter
    def filled_dataset(self, dataset: pd.DataFrame):
        self._filled_dataset = dataset

    def compute_metrics(self, y_true, y_pred):
        &#34;&#34;&#34;
        Compute metrics for regression labels of size nx1
        &#34;&#34;&#34;
        metrics = {}
        # Interpolate predictions to labels, eg convert 0.2 to 0, 0.7 to 1 etc.
        y_pred_interp = self.unique_labs[
            np.abs(np.reshape(self.unique_labs, (-1, 1)) - y_pred).argmin(axis=0)
        ]
        metrics[&#34;confusion_matrix&#34;] = confusion_matrix(y_true, y_pred_interp)
        metrics[&#34;classification_report&#34;] = classification_report(y_true, y_pred)
        return metrics

    @property
    def is_fitted(self) -&gt; bool:
        &#34;&#34;&#34;
        Tries to load model from memory/disk, if it fails, returns False, else returns True
        &#34;&#34;&#34;
        try:
            self.model
        except TrainingRequired:
            return False
        return True

    def train(self) -&gt; None:
        &#34;&#34;&#34;
        - Trains 7 different models, one per each different water security risk.
        - Applies feature selection and generation per different model.
        - Keeps 0.3 validation size, computes classification metrics, saves them, then fits each model to the whole available dataset for each risk.
        - Creates the filled dataset and saves it to disk
        - Creates the prediction mask (what labels from the filled dataset were predicted) and saves it to memory
        &#34;&#34;&#34;
        dataset = self.dataset
        labeled = dataset[self.train_mask]
        model = {}
        train_metrics = {}
        valid_metrics = {}
        filled_dataset = dataset[self.id_columns + self.lab_names].copy()
        for label in self.lab_names:
            train_mask = ~pd.isnull(dataset[label])
            labeled = dataset.loc[train_mask, :]
            train_set, valid_set = train_test_split(
                labeled, test_size=0.3, random_state=RANDOM_SEED
            )

            model[label] = Pipeline(
                [
                    (&#34;FeatureSelection&#34;, FeatureSelectionAndGeneration()),
                    (&#34;Classification&#34;, Classifier()),
                ]
            )
            model[label].fit(train_set[self.feat_names], train_set[label])
            train_preds = model[label].predict(train_set[self.feat_names])
            valid_preds = model[label].predict(valid_set[self.feat_names])
            train_metrics[label] = self.compute_metrics(train_set[label], train_preds)
            valid_metrics[label] = self.compute_metrics(valid_set[label], valid_preds)
            model[label].fit(labeled[self.feat_names], labeled[label])

            filled_dataset.loc[~train_mask, label] = model[label].predict(
                dataset.loc[~train_mask, self.feat_names]
            )
        self.model = model
        self.save_model()
        with open(VALIDATION_METRICS_PATH, &#34;wb&#34;) as out:
            pickle.dump(valid_metrics, out)
        with open(TRAINING_METRICS_PATH, &#34;wb&#34;) as out:
            pickle.dump(train_metrics, out)
        self.filled_dataset = filled_dataset
        self.filled_dataset.to_csv(FILLED_DATASET_PATH, index=False)
        prediction_mask = self.filled_dataset[self.id_columns + self.lab_names]
        prediction_mask[self.lab_names] = pd.isnull(dataset[self.lab_names])
        prediction_mask.to_csv(PREDICTION_MASK_PATH, index=False)
        import data.model.metrics

        importlib.reload(data.model.metrics)
        import data.model.predictions

        importlib.reload(data.model.predictions)

    def test(self, latitude, longitude):
        &#34;&#34;&#34;
        Given a specific latitude and longitude value, either returns saved predictions from the filled dataset, if the point is close to the
        ones that have already been predicted, or uses a REST API to load the country to which the latitude and longitude refer, uses the country data
        to create the feature vector and computes the prediction using the trained models.
        Returns the series of the found labels, which also contain city and country, and the series of booleans which shows which predictions were predicted and which where not.
        &#34;&#34;&#34;
        try:
            from data.model.predictions import FILLED_DATASET, PREDICTION_MASK
        except ImportError:
            raise TrainingRequired(&#34;Filled Dataset&#34;)
        check_existing = FILLED_DATASET.apply(
            lambda x: is_close((latitude, longitude), (x[&#34;latitude&#34;], x[&#34;longitude&#34;])),
            axis=1,
        )
        if np.any(check_existing):
            labs = list(sorted(self.model.keys()))
            return (
                FILLED_DATASET.loc[check_existing, labs + [&#34;city&#34;, &#34;country&#34;]].iloc[0],
                PREDICTION_MASK.loc[check_existing, labs].iloc[0],
            )
        try:
            place = get_place(latitude, longitude)
        except AttributeError:
            raise InvalidCoordinates
        population_density = get_average_1k_population_density(latitude, longitude)

        from data.unlabeled import COUNTRIES_DATASET

        feats = COUNTRIES_DATASET.loc[place[&#34;code&#34;]].copy()
        feats[&#34;population_1k_density&#34;] = population_density
        preds = {}
        mask = {}
        for label in self.model:
            preds[label] = self.model[label].predict(feats)[0]
            mask[label] = True
        preds[&#34;city&#34;] = place[&#34;city&#34;]
        preds[&#34;country&#34;] = place[&#34;country&#34;]
        return pd.Series(preds), pd.Series(mask)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="water_security.classification.model_handler.ModelHandler.dataset"><code class="name">var <span class="ident">dataset</span> : pandas.core.frame.DataFrame</code></dt>
<dd>
<div class="desc"><p>The dataset for the training step.
When it is loaded the first time, several variables are defined:
- lab_names: the labels names/columns of the dataset
- unique_labs: the unique labels values
- feat_names: the features names/columns of the dataset
- train_mask: the mask that refers to the cities that are labeled at least for one risk</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dataset(self) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    The dataset for the training step.
    When it is loaded the first time, several variables are defined:
        - lab_names: the labels names/columns of the dataset
        - unique_labs: the unique labels values
        - feat_names: the features names/columns of the dataset
        - train_mask: the mask that refers to the cities that are labeled at least for one risk
    &#34;&#34;&#34;
    if self._dataset is None:
        from data.labeled.preprocessed import LABELED_CITIES, RISKS_MAPPING
        from data.dataset import DATASET as dataset

        self.lab_names = sorted(RISKS_MAPPING.keys())
        self.unique_labs = np.unique(dataset[self.lab_names].T.stack().values)
        self.feat_names = [
            x
            for x in dataset.columns
            if x not in self.lab_names and x not in self.id_columns
        ]
        self.train_mask = dataset[self.lab_names].apply(
            lambda x: all(pd.isnull(x)), axis=1
        )
        self._dataset = dataset
    return self._dataset</code></pre>
</details>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler.filled_dataset"><code class="name">var <span class="ident">filled_dataset</span> : pandas.core.frame.DataFrame</code></dt>
<dd>
<div class="desc"><p>The dataset that has filled labels, which were produced from the predictions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def filled_dataset(self) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    The dataset that has filled labels, which were produced from the predictions
    &#34;&#34;&#34;
    if self._filled_dataset is None:
        try:
            self._filled_dataset = pd.read_csv(FILLED_DATASET_PATH)
        except IOError:
            raise TrainingRequired(&#34;Filled Dataset&#34;)
    return self._filled_dataset</code></pre>
</details>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler.is_fitted"><code class="name">var <span class="ident">is_fitted</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tries to load model from memory/disk, if it fails, returns False, else returns True</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_fitted(self) -&gt; bool:
    &#34;&#34;&#34;
    Tries to load model from memory/disk, if it fails, returns False, else returns True
    &#34;&#34;&#34;
    try:
        self.model
    except TrainingRequired:
        return False
    return True</code></pre>
</details>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler.model"><code class="name">var <span class="ident">model</span> : sklearn.pipeline.Pipeline</code></dt>
<dd>
<div class="desc"><p>If model is not defined, try to loaded from disk</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model(self) -&gt; Pipeline:
    &#34;&#34;&#34;
    If model is not defined, try to loaded from disk
    &#34;&#34;&#34;
    if self._model is None:
        try:
            from data.model import MODEL

            self._model = MODEL
        except ImportError:
            raise TrainingRequired(&#34;Model&#34;)
    return self._model</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="water_security.classification.model_handler.ModelHandler.compute_metrics"><code class="name flex">
<span>def <span class="ident">compute_metrics</span></span>(<span>self, y_true, y_pred)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute metrics for regression labels of size nx1</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_metrics(self, y_true, y_pred):
    &#34;&#34;&#34;
    Compute metrics for regression labels of size nx1
    &#34;&#34;&#34;
    metrics = {}
    # Interpolate predictions to labels, eg convert 0.2 to 0, 0.7 to 1 etc.
    y_pred_interp = self.unique_labs[
        np.abs(np.reshape(self.unique_labs, (-1, 1)) - y_pred).argmin(axis=0)
    ]
    metrics[&#34;confusion_matrix&#34;] = confusion_matrix(y_true, y_pred_interp)
    metrics[&#34;classification_report&#34;] = classification_report(y_true, y_pred)
    return metrics</code></pre>
</details>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Saves model to memory</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self) -&gt; None:
    &#34;&#34;&#34;
    Saves model to memory
    &#34;&#34;&#34;
    with open(os.path.join(MODEL_PATH), &#34;wb&#34;) as out:
        pickle.dump(self.model, out)
    import data.model

    importlib.reload(data.model)</code></pre>
</details>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self, latitude, longitude)</span>
</code></dt>
<dd>
<div class="desc"><p>Given a specific latitude and longitude value, either returns saved predictions from the filled dataset, if the point is close to the
ones that have already been predicted, or uses a REST API to load the country to which the latitude and longitude refer, uses the country data
to create the feature vector and computes the prediction using the trained models.
Returns the series of the found labels, which also contain city and country, and the series of booleans which shows which predictions were predicted and which where not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test(self, latitude, longitude):
    &#34;&#34;&#34;
    Given a specific latitude and longitude value, either returns saved predictions from the filled dataset, if the point is close to the
    ones that have already been predicted, or uses a REST API to load the country to which the latitude and longitude refer, uses the country data
    to create the feature vector and computes the prediction using the trained models.
    Returns the series of the found labels, which also contain city and country, and the series of booleans which shows which predictions were predicted and which where not.
    &#34;&#34;&#34;
    try:
        from data.model.predictions import FILLED_DATASET, PREDICTION_MASK
    except ImportError:
        raise TrainingRequired(&#34;Filled Dataset&#34;)
    check_existing = FILLED_DATASET.apply(
        lambda x: is_close((latitude, longitude), (x[&#34;latitude&#34;], x[&#34;longitude&#34;])),
        axis=1,
    )
    if np.any(check_existing):
        labs = list(sorted(self.model.keys()))
        return (
            FILLED_DATASET.loc[check_existing, labs + [&#34;city&#34;, &#34;country&#34;]].iloc[0],
            PREDICTION_MASK.loc[check_existing, labs].iloc[0],
        )
    try:
        place = get_place(latitude, longitude)
    except AttributeError:
        raise InvalidCoordinates
    population_density = get_average_1k_population_density(latitude, longitude)

    from data.unlabeled import COUNTRIES_DATASET

    feats = COUNTRIES_DATASET.loc[place[&#34;code&#34;]].copy()
    feats[&#34;population_1k_density&#34;] = population_density
    preds = {}
    mask = {}
    for label in self.model:
        preds[label] = self.model[label].predict(feats)[0]
        mask[label] = True
    preds[&#34;city&#34;] = place[&#34;city&#34;]
    preds[&#34;country&#34;] = place[&#34;country&#34;]
    return pd.Series(preds), pd.Series(mask)</code></pre>
</details>
</dd>
<dt id="water_security.classification.model_handler.ModelHandler.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><ul>
<li>Trains 7 different models, one per each different water security risk.</li>
<li>Applies feature selection and generation per different model.</li>
<li>Keeps 0.3 validation size, computes classification metrics, saves them, then fits each model to the whole available dataset for each risk.</li>
<li>Creates the filled dataset and saves it to disk</li>
<li>Creates the prediction mask (what labels from the filled dataset were predicted) and saves it to memory</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self) -&gt; None:
    &#34;&#34;&#34;
    - Trains 7 different models, one per each different water security risk.
    - Applies feature selection and generation per different model.
    - Keeps 0.3 validation size, computes classification metrics, saves them, then fits each model to the whole available dataset for each risk.
    - Creates the filled dataset and saves it to disk
    - Creates the prediction mask (what labels from the filled dataset were predicted) and saves it to memory
    &#34;&#34;&#34;
    dataset = self.dataset
    labeled = dataset[self.train_mask]
    model = {}
    train_metrics = {}
    valid_metrics = {}
    filled_dataset = dataset[self.id_columns + self.lab_names].copy()
    for label in self.lab_names:
        train_mask = ~pd.isnull(dataset[label])
        labeled = dataset.loc[train_mask, :]
        train_set, valid_set = train_test_split(
            labeled, test_size=0.3, random_state=RANDOM_SEED
        )

        model[label] = Pipeline(
            [
                (&#34;FeatureSelection&#34;, FeatureSelectionAndGeneration()),
                (&#34;Classification&#34;, Classifier()),
            ]
        )
        model[label].fit(train_set[self.feat_names], train_set[label])
        train_preds = model[label].predict(train_set[self.feat_names])
        valid_preds = model[label].predict(valid_set[self.feat_names])
        train_metrics[label] = self.compute_metrics(train_set[label], train_preds)
        valid_metrics[label] = self.compute_metrics(valid_set[label], valid_preds)
        model[label].fit(labeled[self.feat_names], labeled[label])

        filled_dataset.loc[~train_mask, label] = model[label].predict(
            dataset.loc[~train_mask, self.feat_names]
        )
    self.model = model
    self.save_model()
    with open(VALIDATION_METRICS_PATH, &#34;wb&#34;) as out:
        pickle.dump(valid_metrics, out)
    with open(TRAINING_METRICS_PATH, &#34;wb&#34;) as out:
        pickle.dump(train_metrics, out)
    self.filled_dataset = filled_dataset
    self.filled_dataset.to_csv(FILLED_DATASET_PATH, index=False)
    prediction_mask = self.filled_dataset[self.id_columns + self.lab_names]
    prediction_mask[self.lab_names] = pd.isnull(dataset[self.lab_names])
    prediction_mask.to_csv(PREDICTION_MASK_PATH, index=False)
    import data.model.metrics

    importlib.reload(data.model.metrics)
    import data.model.predictions

    importlib.reload(data.model.predictions)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="water_security.classification.model_handler.TrainingRequired"><code class="flex name class">
<span>class <span class="ident">TrainingRequired</span></span>
<span>(</span><span>obj)</span>
</code></dt>
<dd>
<div class="desc"><p>Exception class to raise if estimator is used before fitting.</p>
<p>This class inherits from both ValueError and AttributeError to help with
exception handling and backward compatibility.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from sklearn.svm import LinearSVC
&gt;&gt;&gt; from sklearn.exceptions import NotFittedError
&gt;&gt;&gt; try:
...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
... except NotFittedError as e:
...     print(repr(e))
NotFittedError(&quot;This LinearSVC instance is not fitted yet. Call 'fit' with
appropriate arguments before using this estimator.&quot;...)
</code></pre>
<div class="admonition versionchanged">
<p class="admonition-title">Changed in version:&ensp;0.18</p>
<p>Moved from sklearn.utils.validation.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrainingRequired(NotFittedError):
    def __init__(self, obj):
        super().__init__(f&#34;{obj} could not be loaded. Training model is required&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.exceptions.NotFittedError</li>
<li>builtins.ValueError</li>
<li>builtins.AttributeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="water_security.classification" href="index.html">water_security.classification</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="water_security.classification.model_handler.InvalidCoordinates" href="#water_security.classification.model_handler.InvalidCoordinates">InvalidCoordinates</a></code></h4>
</li>
<li>
<h4><code><a title="water_security.classification.model_handler.ModelHandler" href="#water_security.classification.model_handler.ModelHandler">ModelHandler</a></code></h4>
<ul class="two-column">
<li><code><a title="water_security.classification.model_handler.ModelHandler.compute_metrics" href="#water_security.classification.model_handler.ModelHandler.compute_metrics">compute_metrics</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.dataset" href="#water_security.classification.model_handler.ModelHandler.dataset">dataset</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.filled_dataset" href="#water_security.classification.model_handler.ModelHandler.filled_dataset">filled_dataset</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.is_fitted" href="#water_security.classification.model_handler.ModelHandler.is_fitted">is_fitted</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.model" href="#water_security.classification.model_handler.ModelHandler.model">model</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.save_model" href="#water_security.classification.model_handler.ModelHandler.save_model">save_model</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.test" href="#water_security.classification.model_handler.ModelHandler.test">test</a></code></li>
<li><code><a title="water_security.classification.model_handler.ModelHandler.train" href="#water_security.classification.model_handler.ModelHandler.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="water_security.classification.model_handler.TrainingRequired" href="#water_security.classification.model_handler.TrainingRequired">TrainingRequired</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>